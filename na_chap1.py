# -*- coding: utf-8 -*-
"""NA_chap1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CGAQ9vgNH8xhS26OZVOxi1ZGvAeg04cs
"""

import numpy as np
import numpy.linalg as lg
import scipy.linalg as slg
import matplotlib.pyplot as plt
np.set_printoptions(legacy='1.25')

"""1.1 Introduction"""

# An experiment to show the effects of rounding error. Take a square root several times
# and then square...you should end up where you started, shouldn't you?

x = 16
m = 2
for i in range(m):
    x =  x**.5
print(x)
for i in range(m):
    x = x**2
x

# Example 1.1.1(i)
m = 20
I = np.zeros(m+1)
I[0]=  1-np.exp(-1)
for n in range(m):
        I[n+1] = 1 -(n+1)*I[n]
plt.bar(range(m+1),I)

# Example 1.1.1(i), ctd
m = 30
I2 = np.zeros(m+1)
I2[m]= 0
for n in range(m,0,-1):
        I2[n-1] = (1-I2[n])/(n)
plt.bar(range(m+1),I2)

# A look at all the values
I2

# And with this reverse technique look how accurately we've calculated I_0
I2[0]-1+np.exp(-1)

# Later we'll look at how to compute this integral numerically. Let's see how scipy's built in algorithm works.
from scipy.integrate import quad
def f(x):
  return x**20*np.exp(x-1)
quad(f,0,1)

# Compare it with the reverse recurrence
I2[20]

"""1.2 Matrix Basics"""

# Example 1.2.1
A = np.array([[2,1,3,4],[0,1,5,6],[1,7,1,3],[9,8,2,4]])
print(A)

# Colon notation in Python works differently from the notation given in the notes.
# Indexing starts from 0 but note that the upper limit is one bigger than we might expect.
A[2,0:4]

A[3,1:3]

A[:,[0]]

A[1:4,:]

# Some matrix basics...
np.transpose(A)

A.T

x=np.array([[-1],[-1],[2],[1]])
x

# What can we deduce from the following result?
# (Notice how we define matrix-vector multiplication...it works for matrix-matrix, too).
x.T@A@x

# ...let's change the way this is output.
QF= (x.T@A@x).item(0)
print(QF)

# Determinant...why isn't this a whole number?
lg.det(A)

# Matrix inverse.
lg.inv(A)

# Note that scipy and numpy can give ever so slightly different answers. This should have no material effect.
slg.det(A)

slg.inv(A)

slg.inv(A)-lg.inv(A)

# Some useful commands
np.triu(A)

np.tril(A)

I4=np.eye(4)

np.diag([3,2,-1,4])

A.T@A

I4.T@I4

"""1.3 Norms"""

# Examples of vector norms
x=np.array([[-1],[-1],[2],[1]])
print(x)
print(lg.norm(x,1))
print(lg.norm(x,np.inf))
print(lg.norm(x,2))

np.sqrt(7)

# Examples of matrix norms
A = np.array([[2,1,3,4],[0,1,5,6],[1,7,1,3],[9,8,2,4]])
print(A)
print(lg.norm(A,1))
print(lg.norm(A,np.inf))
print(lg.norm(A,2))
print(lg.norm(A,'fro'))

"""1.4 Gaussian Elimination"""

# Forward substitution algorithm
def forsub(L, b):
    n = L.shape[0]  # Gives us the dimension of our matrix
    x = np.zeros((n,1)) # We'll use this vector to hold our solution.
    for i in range(n):  # i = 0, 1,..., n-1
        S = b[i]
        for k in range(i): # k = 0, 1,..., i-1
            S = S - L[i,k]*x[k]
        x[i] = S/L[i,i]
    return x

# Test it out.
L=np.array([[5,0,0,0],[6,-2,0,0],[-2,0,-1,0],[6,2,7,-2]])
b=np.array([5,2,1,-9]).T
forsub(L,b)

# Check it worked
L@forsub(L,b)

# Backward substitution algorithm
def backsub(U, b):
    n = U.shape[0]
    x = np.zeros((n,1)) # We'll use this vector to hold our solution.
    for i in range(n-1,-1,-1): # Count backwards
        S = b[i]
        for k in range(n-1,i-1,-1): # Can you see why this indexing is correct?
            S = S - U[i,k]*x[k]
        x[i] = S/U[i,i]
    return x

# Testing it out. Notice that we have set the first element of U to 4.0. Why not 4?
U=np.array([[4.0,4,-5,5],[0,-1,0,0],[0,0,3,2],[0,0,0,4]])
b = np.array([8,0,5,4]).T
backsub(U,b)

U@backsub(U,b)

# Basic Gaussian elimination. No pivoting implemented.
def GE(A, b):
    A0 = np.copy(A) # It is essential that we make a copy of our original data. Why?
    b0 = np.copy(b)
    n = A0.shape[0]
    for i in range(n-1):
        for j in range(i+1,n):
            M = A0[j,i]/A0[i,i]   # The multiplier. We don't need to store this in a separate variable.
            A0[j,i] = 0
            for k in range(i+1,n):
                A0[j,k]= A0[j,k] - M*A0[i,k]
            b0[j] = b0[j] - M*b0[i]
    x = backsub(A0,b0) # A0 is in uppertriangular form so we can use backsub.
    return x

# Test
A=np.array([[2.0,4,-4,1],[-1,1,2,3],[3,6,1,-2],[1,1,-4,1]])
b = np.array([0,4,-7,2.0]).T
GE(A,b)

A@GE(A,b)

# How fast is this algorithm? Try for a number of different dimensions.
from time import time
rng = np.random.default_rng()
n = 100
A = rng.uniform(low=-10, high=10, size=[n,n])
b = rng.uniform(low=-10, high=10, size=[n,1])
t0 = time()
GE(A,b)
t1 = time()
t1-t0

# lu is scipy's implementation...it works a lot more quickly!
from time import time
rng = np.random.default_rng()
n = 100
A = rng.uniform(low=-10, high=10, size=[n,n])
t0 = time()
pl, u = slg.lu(A, permute_l=True)
t1 = time()
t1-t0

"""1.4.2 Pivoting"""

# Implementing partial pvioting.
def GEPP(A, b):
    n = A.shape[0]
    A0 = np.copy(A)
    b0 = np.copy(b)
    for i in range(n-1):
        pr = abs(A0[i:,i]).argmax() + i # The pivot row. Why do we add i?
        A0[[pr,i],i:] = A0[[i,pr],i:] # Pivot the matrix
        b0[[pr,i]] = b0[[i,pr]]       # Pivot the righthand side
        for j in range(i+1,n):
            M = A0[j,i]/A0[i,i]
            A0[j,i] = 0
            for k in range(i+1,n):
                A0[j,k]= A0[j,k] - M*A0[i,k]
            b0[j] = b0[j] - M*b0[i]
    x = backsub(A0,b0)
    return x

# Test it out.
A=np.array([[2.0,4,-4,1],[-1,1,2,3],[3,6,1,-2],[1,1,-4,1]])
b = np.array([0,4,-7,2.0]).T
GEPP(A,b)

# Example 1.4.4 from the notes.
A=np.array([[-2.0,-1,6,6],[1,-1,6,0],[-1,1,2,1],[-2,-3,2,-2]])
b = np.array([23,5,5,5.0]).T
GEPP(A,b)

# Swap rows 2 and 3 from first example. Answer doesn't change.
A=np.array([[2.0,4,-4,1],[3,6,1,-2],[-1,1,2,3],[1,1,-4,1]])
b = np.array([0,-7,4,2.0]).T
GEPP(A,b)

# But what happens without pivoting?
GE(A,b)

# GE with complete pivoting
def GECP(A, b):
    A0 = np.copy(A)
    b0 = np.copy(b)
    n = A.shape[0]
    ind = np.array(range(n))
    for i in range(n-1):
        B = abs(A0[i:,i:]) # Extract the part of the matrix we're still working with. We want biggest element ignoring signs.
        piv =np.unravel_index(np.argmax(B, axis=None), B.shape) # The syntax for finding the pivot element is a bit messy.
        pr = piv[0]+i
        pc = piv[1]+i
        A0[[pr,i],i:] = A0[[i,pr],i:]  # Swap rows
        b0[[pr,i]] = b0[[i,pr]]        # Need to swap rhs
        A0[:,[pc,i]] = A0[:,[i,pc]]    # Swap columns
        ind[[pc,i]] = ind[[i,pc]]      # This requires us to swap ordering of the solution vector
        for j in range(i+1,n):         # Then eliminate
            M = A0[j,i]/A0[i,i]
            A0[j,i] = 0
            for k in range(i+1,n):
                A0[j,k]= A0[j,k] - M*A0[i,k]
            b0[j] = b0[j] - M*b0[i]
    x = backsub(A0,b0)
    return x[np.argsort(ind)]

A=np.array([[2.0,4,-4,1],[3,6,1,-2],[-1,1,2,3],[1,1,-4,1]])
b = np.array([0,-7,4,2.0]).T
GECP(A,b)

A=np.array([[-2.0,-1,6,6],[1,-1,6,0],[-1,1,2,1],[-2,-3,2,-2]])
b = np.array([23,5,5,5.0]).T
GECP(A,b)

# How do timings compare?
from time import time
rng = np.random.default_rng()
n = 100
A = rng.uniform(low=-10, high=10, size=[n,n])
b = rng.uniform(low=-10, high=10, size=[n,1])
t0 = time()
GE(A,b)
t1 = time()
print(t1-t0)
t0 = time()
GEPP(A,b)
t1 = time()
print(t1-t0)
t0 = time()
GECP(A,b)
t1 = time()
t1-t0

"""1.4.3 Matrix Factorisation"""

# LU factorisation
def LU(A):
    n = A.shape[0]
    U = np.copy(A) # We'll eliminate from A to find U
    L = np.eye(n)  # L has a unit diagonal
    for i in range(n-1):
        for j in range(i+1,n):
            M = U[j,i]/U[i,i]
            U[j,i] = 0
            for k in range(i+1,n):
                U[j,k]= U[j,k] - M*U[i,k]
            L[j,i] = M
    return L,U

A=np.array([[2.0,4,-4,1],[-1,1,2,3],[3,6,1,-2],[1,1,-4,1]])
L,U=LU(A)
print(L)
print(U)

# Check
L@U-A

# Timing
from time import time
rng = np.random.default_rng()
n = 100
A = rng.uniform(low=-10, high=10, size=[n,n])
t0 = time()
LU(A)
t1 = time()
t1-t0

"""1.4.4 Cholesky Factorisation"""

# Cholesky factorisation
def Chol(A):
    n = A.shape[0]
    L = np.zeros([n,n])
    for i in range(n):
        S = A[i,i]
        for j in range(i):
            S = S - L[i,j]**2
        L[i,i] = np.sqrt(S)
        for j in range(i+1,n):
            M = 0
            for k in range(i):
                M = M + L[i,k]*L[j,k]
            L[j,i]= (A[j,i] - M)/L[i,i]
    return L

A=np.array([[9,3,-3],[3,5,-5],[-3,-5,14]])
L=Chol(A)

L@L.T

L

# Test it on a bigger example
n=20
rng = np.random.default_rng()
A = rng.uniform(low=-10, high=10, size=[n,n])
A=A@A.T
L=Chol(A)
lg.norm(L@L.T-A,1)

# But it won't work for all symmetric matrices
n = 5
A = rng.uniform(low=-10, high=10, size=[n,n])
A=A+A.T
Chol(A)

# Timing
from time import time
rng = np.random.default_rng()
n = 100
A = rng.uniform(low=-10, high=10, size=[n,n])
A=A@A.T
t0 = time()
Chol(A)
t1 = time()
print(t1-t0)
t0 = time()
LU(A)
t1 = time()
t1-t0

# Faster Cholesky factorisation:
def Chol2(A):
    n = A.shape[0]
    L = np.zeros([n,n])
    for i in range(n):
        S = A[i,i] - L[i,0:i]@L[i,0:i].T
        L[i,i] = np.sqrt(S)
        for j in range(i+1,n):
            M = L[i,0:i]@L[j,0:i].T
            L[j,i]= (A[j,i] - M)/L[i,i]
    return L

# Timing
from time import time
rng = np.random.default_rng()
n = 250
A = rng.uniform(low=-10, high=10, size=[n,n])
A=A@A.T
t0 = time()
Chol(A)
t1 = time()
print(t1-t0)
t0 = time()
Chol2(A)
t1 = time()
print(t1-t0)
t0 = time()
lg.cholesky(A)
t1 = time()
t1-t0

"""1.5 Accuracy of Gaussian Elimination"""

# Condition numbers. What happens as we change n?
n = 10
A = rng.uniform(low=-10, high=10, size=[n,n])
print(lg.cond(A))
print(lg.cond(A,np.inf))
print(lg.cond(A,1))

# How accurate is our solution? Let's look at forward error.
x=np.array(range(n)).reshape(n,1)
b=A@x
xhat = GEPP(A,b)
lg.norm(xhat-x)/lg.norm(xhat)

# Now backward error
lg.norm(A@xhat-b)/(lg.norm(A)*lg.norm(xhat)+lg.norm(b))

# Here's a way of getting larger condition numbers. Increase m to see what happens.
n = 100
m = 3
A = np.random.rand(n,n)
A = lg.matrix_power(A,m)
x = np.array(range(n)).reshape(n,1)
b = A@x
lg.cond(A)

# Compare forward and backward errors.
xhat = GEPP(A,b)
print(lg.norm(xhat-x)/lg.norm(xhat))
print(lg.norm(A@xhat-b)/(lg.norm(A)*lg.norm(xhat)+lg.norm(b)))

# Does pivoting help?
xhat = GE(A,b)
print(lg.norm(xhat-x)/lg.norm(xhat))
print(lg.norm(A@xhat-b)/(lg.norm(A)*lg.norm(xhat)+lg.norm(b)))

# And complete pivoting?
xhat = GECP(A,b)
print(lg.norm(xhat-x)/lg.norm(xhat))
print(lg.norm(A@xhat-b)/(lg.norm(A)*lg.norm(xhat)+lg.norm(b)))

"""1.6 Iterative Methods"""

# Jacobi iteration. Look how we implement the summations: as scalar products.
def Jacobi(A,b,x0,N):
    xk = np.copy(x0)
    n = A.shape[0]
    for k in range(N):
        xn = np.zeros([n,1])
        for i in range(n):
            xn[i] = (b[i] - A[i,:i]@xk[:i] - A[i,i+1:]@xk[i+1:])/A[i,i]
        xk = xn # We need to keep 2 vectors on the go (since we're using old values as we create the new vector)
        print(xk)
    return xk

# Example 1.6.1
A=np.array([[5.,1,-1],[1,4,0],[1,0,5]])
b=np.array([[5.],[5],[6]])
Jacobi(A,b,np.array([[0.],[0],[0]]),8)

# Gauss-Seidel
def GS(A,b,x0,N):
    xk = np.copy(x0)
    n = A.shape[0]
    for k in range(N):
        for i in range(n):
            xk[i] = (b[i] - A[i,:i]@xk[:i] - A[i,i+1:]@xk[i+1:])/A[i,i] # We only need one vector
        print(xk)
    return xk

# Example 1.6.2(i) (with a slightly different x0)
A=np.array([[5.,1,-1],[1,4,0],[1,0,5]])
b=np.array([[5.],[5],[6]])
GS(A,b,np.array([[0.],[0],[-1.0]]),7)

# Example 1.6.2(ii)
A=np.array([[4.,-2,1],[-2,10,0],[1,0,5]])
b=np.array([[3.],[18],[16]])
GS(A,b,np.array([[3.],[2],[1]]),6)

# Successive over-relaxation
def SOR(A,b,x0,w,N):
    xk = np.copy(x0)
    n = A.shape[0]
    for k in range(N):
        for i in range(n):
            xk[i] = w*(b[i] - A[i,:i]@xk[:i] - A[i,i+1:]@xk[i+1:])/A[i,i] + (1-w)*xk[i]
        print(xk)
    return xk

# Example 1.6.3. See what happens when you change w.
A=np.array([[10.,-4,-4],[-4,5,0],[-4,0,10]])
b=np.array([[2.],[1],[6]])
SOR(A,b,np.array([[0.],[0],[0]]),0.5,10)

"""1.6.5 Stationary Iterative Methods"""

# Matrix representations of stationary iterative methods
def SIM(A,b,x0,N,method,w = 0):
    xk = np.copy(x0)
    n = A.shape[0]
    D = np.diag(np.diag(A))
    L = np.tril(A,-1)
    U = np.triu(A,1)
    if method == "G":
        w = 1
    for k in range(N):
        if method == "J":
            # Jacobi
            xk = backsub(D,b-(L+U)@xk)
        else:
            # SOR (or GS if w=1)
            xk = forsub(D/w+L,b-((1-1/w)*D+U)@xk)
        print(xk)
    return xk

A=np.array([[10.,-4,-4],[-4,5,0],[-4,0,10]])
b=np.array([[2.],[1],[6]])
SIM(A,b,np.array([[0.],[0],[0]]),10,"G")

# Now we've added a stopping criterion. If w = 0 we use Jacobi, otherwise SOR.
def SIM2(A,b,x0,w=0,TOL=1e-3):
    k = 0
    xk = np.copy(x0)
    n = A.shape[0]
    D = np.diag(np.diag(A))
    L = np.tril(A,-1)
    U = np.triu(A,1)
    while lg.norm(b-A@xk) > TOL:
        if w == 0:
            # Jacobi
            xk = backsub(D,b-(L+U)@xk)
        else:
            # SOR (or GS if w=1)
            xk = forsub(D/w+L,b-((1-1/w)*D+U)@xk)
        k = k + 1
    return xk, k

A=np.array([[10.,-4,-4],[-4,5,0],[-4,0,10]])
b=np.array([[2.],[1],[6]])
SIM2(A,b,np.array([[0.],[0],[0]]))

"""1.6.6 Convergence"""

n=20
rng = np.random.default_rng()
A = rng.uniform(low=-10, high=10, size=[n,n])
A=A@A.T+np.eye(n)*20
b = rng.uniform(low=-10, high=10, size=[n,1])
xk,k=SIM2(A,b,np.zeros((n,1)),0)
print(k)
xk,k=SIM2(A,b,np.zeros((n,1)),1)
print(k)
xk,k=SIM2(A,b,np.zeros((n,1)),1.2)
print(k)

# Rate of convergence of GS
D = np.diag(np.diag(A))
L = np.tril(A,-1)
U = np.triu(A,1)
l,v=lg.eig(lg.inv(D+L)@(U))
max(abs(l))

# Rate of convergence of SOR with w = 1.2
w=1.2
l,v=lg.eig(lg.inv(D/w+L)@(U+(1-1/w)*D))
max(abs(l))

l,v=lg.eig(lg.inv(D)@(L+U))
max(abs(l))

"""1.6.7 Conjugate Gradient Iteration"""

# Steepest descent
def SD(A,b,x0,TOL):
    k = 0
    xk = np.copy(x0)
    rk = b - A@xk
    while lg.norm(rk) > TOL:
        k = k + 1
        ak = rk.T@rk/(rk.T@A@rk)
        xk = xk + ak*rk
        rk = b - A@xk
    return xk,k

SD(A,b,np.zeros((n,1)),1e-3)

# The conjugate gradient method
def CG(A,b,x0,TOL):
    k = 0
    xk = np.copy(x0)
    rk = b - A@xk
    pk = rk
    while lg.norm(rk) > TOL:
        k = k + 1
        rko = np.copy(rk)
        ak = rk.T@rk/(pk.T@A@pk)
        ak= ak.item(0)
        xk = xk + ak*pk
        rk = rk - ak*A@pk
        bk = rk.T@rk/(rko.T@rko)
        bk = bk.item(0)
        pk = rk + bk*pk
    return xk,k

CG(A,b,np.zeros((n,1)),1e-3)

# A convergence estimate
D=np.diag(np.diag(A))
kappa=lg.cond(lg.inv(D)@A@lg.inv(D))
(kappa-1)/(kappa+1)

# A bigger example
n = 100
from scipy.sparse import diags
e = np.ones(n)  # vector (1, 1, ..., 1) of length n-1
e0 = np.ones(n-1)  # vector (1, 1, ..., 1) of length n-2
A = diags([-e0, 2.2*e, -e0], [-1, 0, 1]).todense()
b = A@np.ones((n,1))
x,k=CG(A,b,np.zeros((n,1)),1e-5)
k

lg.cond(A)

D = np.diag(np.diag(A))
L = np.tril(A,-1)
U = np.triu(A,1)
l,v=lg.eig(lg.inv(D+L)@(U))
max(abs(l))

x,k=SIM2(A,b,np.zeros((n,1)),1)
k

